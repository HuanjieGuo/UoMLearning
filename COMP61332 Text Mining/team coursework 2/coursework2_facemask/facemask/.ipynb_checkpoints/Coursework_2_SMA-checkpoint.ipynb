{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoEncWHm5s28"
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1V7EzFr6N7p"
   },
   "source": [
    "## Library installation/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1B0__6AYci7"
   },
   "source": [
    "Install and import libraries that are used in multiple sections of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6K2gnqqfRsoS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s7ivGrSR5NaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.0 in /opt/anaconda3/lib/python3.8/site-packages (3.0.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (2.0.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (3.0.5)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (2.11.2)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (1.7.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (0.7.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (1.18.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (4.47.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (2.24.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (2.4.0)\n",
      "Requirement already satisfied: pathy in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (0.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (2.0.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (3.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (20.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (49.2.0.post20200714)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from spacy==3.0) (8.0.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy==3.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy==3.0) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==3.0) (1.25.9)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from pathy->spacy==3.0) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy==3.0) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy==3.0) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_S1J4oc6laA"
   },
   "source": [
    "## Configuring Twitter API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_k-DQXnYXyIO"
   },
   "source": [
    "Please note that the API keys below are the course leader's own API keys. You are allowed to use it to do some small tests, but please be careful because all students in the class now have a copy of it, and hence the limits can be easily exceeded.\n",
    "\n",
    "If your group has decided to use Twitter data, you can [apply for your own keys](https://developer.twitter.com/en/apply-for-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X0v0dZ8fCnqI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8f7bf774be0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hPXFUlTG4BOMjzJpNOtT3BHn9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Bo59uO5Z2a7fxDMmWAts1O6GurF73iIdHnYYzFzgeqew0Oe17P'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1844121-0QhEfcPyRbBWPdOPg4EsGaRZLT7LYQO5jd4rFJmW9k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IUCjfGxH1vUgOsYpFZX0KqHm3K2MUGhNLphQQY5ncGaII'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "auth = tweepy.OAuthHandler('hPXFUlTG4BOMjzJpNOtT3BHn9', 'Bo59uO5Z2a7fxDMmWAts1O6GurF73iIdHnYYzFzgeqew0Oe17P')\n",
    "auth.set_access_token('1844121-0QhEfcPyRbBWPdOPg4EsGaRZLT7LYQO5jd4rFJmW9k', 'IUCjfGxH1vUgOsYpFZX0KqHm3K2MUGhNLphQQY5ncGaII')\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sBB2guxczux"
   },
   "source": [
    "## Downloading of new data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5EwgFk86EfS"
   },
   "source": [
    "**IMPORTANT NOTE**: Please do not run the cell below unless intending to download a new data set.\n",
    "\n",
    "Make sure that you change the parameters.\n",
    "\n",
    "Also, check the [Tweepy API reference](https://docs.tweepy.org/en/latest/api.html) to find out about other ways through which you can retrieve tweets, e.g., by specifying usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSQXfQVGCpmj"
   },
   "outputs": [],
   "source": [
    "# Collect tweets\n",
    "query = \"#notoracism\" + \" -filter:retweets\"\n",
    "cutoff_date = \"2021-01-01\"\n",
    "tweets = tweepy.Cursor(api.search, q=query, lang=\"en\", since=cutoff_date).items(1000)\n",
    "\n",
    "tweets_list = [[tweet.created_at, tweet.user.screen_name, tweet.user.location, tweet.text] for tweet in tweets]\n",
    "\n",
    "tweets_df = pd.DataFrame(data=tweets_list, columns=['date', 'user', 'location', 'text'])\n",
    "\n",
    "# A good idea to save downloaded tweets as CSV\n",
    "tweets_df.to_csv ('current_set.csv', quotechar='\"', encoding='utf8', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qumfu0NwYKqR"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giyVyGPAYUTK"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K23I1qBNZ_HQ"
   },
   "source": [
    "Below we provide some code for text cleaning. However, we encourage you to think of other ways to clean your data, e.g., by removing hashtags, removing usernames, removing duplicate tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWzLqibrM0pa"
   },
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Uncomment the line below if loading from previously saved CSV\n",
    "tweets_df = pd.read_csv('current_set.csv', quotechar='\"', encoding='utf8')\n",
    "\n",
    "# Remove punctuation\n",
    "tweets_df['text_processed'] = tweets_df['text'].map(lambda x: re.sub('[,\\\\.!?]', ' ', x))\n",
    "\n",
    "# Remove unnecessary line breaks\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: re.sub(r\"\\n\", '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "tweets_df['text_processed'] = tweets_df['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Think of how else your data can be cleaned\n",
    "\n",
    "# Print out the first rows \n",
    "tweets_df['text_processed'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22qj0jq3YklZ"
   },
   "source": [
    "## Exploration using a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IIh30BjaWji"
   },
   "source": [
    "Generating a word cloud is one way by which you can check whether your data needs any further cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqxuMbjk8EkA"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# join the words of the different tweets together into one string\n",
    "long_string = ' '.join(unique_tweets)\n",
    "new_long_string = ' '.join(set(long_string.split(\" \")))\n",
    "\n",
    "# create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# generate a word cloud\n",
    "wordcloud.generate(new_long_string)\n",
    "\n",
    "# visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQyNo3FYYuED"
   },
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwAZb_l-7C-L"
   },
   "outputs": [],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tfx5EMNw_pIw"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Do you want to modify this by adding more stop words?\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "  for sentence in sentences:\n",
    "    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "  return [[word for word in simple_preprocess(str(doc)) \n",
    "    if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = tweets_df.text_processed.values.tolist()\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "\n",
    "# create a dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# create a corpus\n",
    "texts = data_words\n",
    "\n",
    "# convert the corpus into a BoW representation\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOb8cNW3PVfk"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# set number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# build an LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "# print keywords in each topic\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RPtA2Yg7Hnm"
   },
   "outputs": [],
   "source": [
    "# visualise the topics\n",
    "!pip install pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWueCQawPlXn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./'+str(num_topics))\n",
    "\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "  pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "  LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7UoGTRZY1pg"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3RLsS3Ac8v7"
   },
   "source": [
    "This implementation is based on the lexicon- and rule-based [VADER](https://github.com/cjhutto/vaderSentiment) sentiment analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gas4oUUIY8iF"
   },
   "outputs": [],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hK8WbOHbA78"
   },
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "for tweet_text in unique_tweets:\n",
    "    vs = analyzer.polarity_scores(tweet_text)\n",
    "    print(tweet_text + '\\t' + str(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qT11HUQBY47n"
   },
   "source": [
    "# Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcanVP2udd24"
   },
   "source": [
    "This implementation is based on [spaCy's model](https://spacy.io/models/en#en_core_web_trf) using contextualised embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUWgWiLZYaBS"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-transformers\n",
    "!python -m spacy download en_core_web_trf\n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "\n",
    "nlp = spacy.load('en_core_web_trf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAvsa90Gpwna"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  for ne in doc.ents:\n",
    "    print('\\tNE found: ', ne.start_char, ne.end_char, ne.label_, tweet_text[ne.start_char:ne.end_char])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Ub0m8zoZfH"
   },
   "source": [
    "# Named Entity Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqLYLoM7ecLP"
   },
   "source": [
    "This implementation is based on [spaCy Entity Linker](https://github.com/egerber/spacy-entity-linker)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvKRih-dYEDL"
   },
   "outputs": [],
   "source": [
    "!pip install spacy-entity-linker\n",
    "!python -m spacyEntityLinker \"download_knowledge_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7bxxYpFcuoz"
   },
   "outputs": [],
   "source": [
    "from spacyEntityLinker import EntityLinker\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory(\n",
    "   \"entityLinker\"\n",
    ")\n",
    "def create_linker(nlp, name):\n",
    "  return EntityLinker()\n",
    "\n",
    "#add to pipeline\n",
    "nlp.add_pipe('entityLinker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnZZ0t32oCgZ"
   },
   "outputs": [],
   "source": [
    "for tweet_text in unique_tweets:\n",
    "  doc = nlp(tweet_text)\n",
    "  print(tweet_text)\n",
    "  all_linked_entities = doc._.linkedEntities\n",
    "  all_linked_entities.pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Coursework 2: SMA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
