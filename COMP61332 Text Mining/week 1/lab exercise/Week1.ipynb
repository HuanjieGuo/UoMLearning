{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0nDXshqOzjL"
   },
   "source": [
    "# Week 1: Sentence segmentation, Tokenisation and Producing Annotations\n",
    "### COMP61332: Text Mining, School of Computer Science, University of Manchester (Riza Batista-Navarro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU-UY7yMOzjR"
   },
   "source": [
    "In this lab session, you will try out some Python code based on the **spaCy** library (https://spacy.io/) for the NLP tasks discussed in the Week 1 Lecture.\n",
    "After this session, you should be able to:\n",
    "- apply **sentence segmentation** on text and possibly improve it by customisation\n",
    "- apply **tokenisation** on text and possibly improve it by customisation\n",
    "- generate machine-readable **annotations** over text\n",
    "\n",
    "The document that you are currently reading is a **Jupyter notebook**. You will receive a copy of it in the form of a file called *Week1.ipynb*, which has been made available to you via Blackboard. Once you have downloaded this file, you should upload it on your Jupyter dashboard (a tab in your web browser that automatically opens when you run Jupyter).\n",
    "\n",
    "Each of the boxes you see below is called a *cell*, which contains Python code that you can:\n",
    "- edit, by typing anywhere inside the cell, or\n",
    "- run, by clicking on the *Run* button, while your cursor is inside the cell.\n",
    "\n",
    "The output of the code will then appear right below the cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3MdBz6dOzjR"
   },
   "source": [
    "## Preparation of necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27283,
     "status": "ok",
     "timestamp": 1612522548300,
     "user": {
      "displayName": "Viktor Schlegel",
      "photoUrl": "",
      "userId": "17445449191599149421"
     },
     "user_tz": 0
    },
    "id": "tbtx2hxFOzjS",
    "outputId": "cff1c55c-9204-4cd3-9d47-3ac9a08333d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/guohuanjie/Desktop/en_core_web_sm-3.0.0\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pathy in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.50.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from pathy->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/tm/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-3.0.0-py3-none-any.whl size=13704311 sha256=d4b36aa55ca319e163bddd4714fbc5a0cafd497353b1f71e0f42ced557459688\n",
      "  Stored in directory: /Users/guohuanjie/Library/Caches/pip/wheels/e9/4f/a5/5610f39f86b40217694eaf94bc7bd929ff9191d27cfa8f73ee\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Attempting uninstall: en-core-web-sm\n",
      "    Found existing installation: en-core-web-sm 3.0.0\n",
      "    Uninstalling en-core-web-sm-3.0.0:\n",
      "      Successfully uninstalled en-core-web-sm-3.0.0\n",
      "Successfully installed en-core-web-sm-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# Loading\n",
    "# !pip install spacy==3.0\n",
    "# !python -m spacy download en_core_web_sm\n",
    "!pip install /Users/guohuanjie/Desktop/en_core_web_sm-3.0.0\n",
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import Sentencizer\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "from spacy.pipeline import EntityRecognizer\n",
    "from spacy import displacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghdx0ZY9OzjS"
   },
   "source": [
    "# File loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zn6qm5w_OzjT"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "def load_file(path):\n",
    "    recipe_text = ''\n",
    "    recipe = codecs.open(path, 'r', encoding = 'utf-8')\n",
    "    recipe_lines = recipe.readlines()\n",
    "    for line in recipe_lines:\n",
    "        recipe_text = recipe_text + line\n",
    "    recipe.close()\n",
    "    return recipe_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUUtuN2LOzjT"
   },
   "source": [
    "# Default sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "F7NxXi2DOzjT",
    "outputId": "9c75d834-3959-490f-a89a-c1de8c46eaed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INGREDIENTS\n",
      "For the pizza base: butter, ghee or coconut oil, for greasing; 140g cauliflower (about 1/4 of a head without the stalk); 1 egg white, beaten; 50g  ground almonds; 40g buckwheat flour; 1/2 tsp sea salt; 1/2 tsp black pepper; 1/4 tsp bicarbonate of soda\n",
      "For the topping: 1 medium mozzarella ball; 2 handfuls fresh tomatoes (a mixture of colours look good); chilli flakes (optional); handful fresh basil; drizzle of olive oil, to serve\n",
      "METHOD\n",
      "1.\n",
      "Preheat the oven to 170C/190C fan/350F/Gas 4.\n",
      "Line a baking tray with baking parchment... lightly grease with butter, ghee or coconut oil.\n",
      "2.\n",
      "Grate the cauliflower into rice-sized pieces using a hand grater or food processor.\n",
      "3.\n",
      "Put all the pizza base ingredients in a bowl... mix well with a spoon, or add to the food processor and blend, to form a sticky dough.\n",
      "4.\n",
      "Using the back of a spoon, spread the dough out onto the greased parchment on the tray, shaping it into a circle 30cm/12in wide.\n",
      "5.\n",
      "Bake in the oven for about five minutes; flip it over and return to the oven for another two minutes.\n",
      "6.\n",
      "Pull the mozzarella ball apart into small pieces and arrange over the base of the pizza.\n",
      "7.\n",
      "Slice the tomatoes into 1cm thick slices and arrange over the pizza; add the chilli flakes, if desired.\n",
      "8.\n",
      "Bake the pizza for 10-15 minutes more until the mozzarella is melted and bubbling.\n",
      "9.\n",
      "Sprinkle the basil over the top of the pizza before serving and drizzle with olive oil.\n",
      "10.\n",
      "Enjoy!\n",
      "Bon appetit!\n"
     ]
    }
   ],
   "source": [
    "# Create a new NLP pipeline,. Specifying English as the language of interest so that English models are loaded.\n",
    "nlp = English()\n",
    "\n",
    "# Create a sentence segmentation component. \n",
    "sentencizer = Sentencizer(punct_chars=[\",\"])\n",
    "\n",
    "# Add the component to the pipeline.\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "text = load_file('CauliflowerPizza.txt')\n",
    "# The following line applies the pipeline (so far only sentence segmentation) on the given text, and stores the result in doc.\n",
    "annotations = nlp(text)\n",
    "\n",
    "# Check the result of sentence segmentation.\n",
    "sents_list = []\n",
    "for sent in annotations.sents:\n",
    "    sents_list.append(sent.text.strip())\n",
    "for sent in sents_list:\n",
    "    print(sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I16cDoECOzjU"
   },
   "source": [
    "### Activity 1a: In the code below (a bit similar to the one above), modify the list of punctuation characters being used by the sentence segmentation component and observe how the result is affected.\n",
    "\n",
    "**Write down any observations here:**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Activity 1b: Feel free to try another your own piece of text as input. \n",
    "\n",
    "**Write down any observations here:**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fiut1LsOzjV"
   },
   "source": [
    "# Custom sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4HBuX0naOzjV",
    "outputId": "e5ebd303-cd52-4944-cd07-c61b7a430658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INGREDIENTS\n",
      "For the pizza base: butter, ghee or coconut oil, for greasing;\n",
      "140g cauliflower (about 1/4 of a head without the stalk);\n",
      "1 egg white, beaten;\n",
      "50g  ground almonds;\n",
      "40g buckwheat flour;\n",
      "1/2 tsp sea salt;\n",
      "1/2 tsp black pepper;\n",
      "1/4 tsp bicarbonate of soda\n",
      "For the topping: 1 medium mozzarella ball;\n",
      "2 handfuls fresh tomatoes (a mixture of colours look good);\n",
      "chilli flakes (optional);\n",
      "handful fresh basil;\n",
      "drizzle of olive oil, to serve\n",
      "METHOD\n",
      "1.\n",
      "Preheat the oven to 170C/190C fan/350F/Gas 4.\n",
      "Line a baking tray with baking parchment... lightly grease with butter, ghee or coconut oil.\n",
      "2.\n",
      "Grate the cauliflower into rice-sized pieces using a hand grater or food processor.\n",
      "3.\n",
      "Put all the pizza base ingredients in a bowl... mix well with a spoon, or add to the food processor and blend, to form a sticky dough.\n",
      "4.\n",
      "Using the back of a spoon, spread the dough out onto the greased parchment on the tray, shaping it into a circle 30cm/12in wide.\n",
      "5.\n",
      "Bake in the oven for about five minutes;\n",
      "flip it over and return to the oven for another two minutes.\n",
      "6.\n",
      "Pull the mozzarella ball apart into small pieces and arrange over the base of the pizza.\n",
      "7.\n",
      "Slice the tomatoes into 1cm thick slices and arrange over the pizza;\n",
      "add the chilli flakes, if desired.\n",
      "8.\n",
      "Bake the pizza for 10-15 minutes more until the mozzarella is melted and bubbling.\n",
      "9.\n",
      "Sprinkle the basil over the top of the pizza before serving and drizzle with olive oil.\n",
      "10.\n",
      "Enjoy! Bon appetit!\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "# Create a new NLP pipeline,. Specifying English as the language of interest so that English models are loaded.\n",
    "nlp = English()\n",
    "\n",
    "config = {\"punct_chars\": [\".\", \"?\", \";\"]}\n",
    "\n",
    "# Add the component to the pipeline.\n",
    "sentencizer = nlp.add_pipe('sentencizer', config=config)\n",
    "\n",
    "text = load_file('CauliflowerPizza.txt')\n",
    "\n",
    "# The following line applies the pipeline (so far only sentence segmentation) on the given text, and stores the result in doc.\n",
    "annotations = nlp(text)\n",
    "\n",
    "# Check the result of sentence segmentation.\n",
    "sents_list = []\n",
    "for sent in annotations.sents:\n",
    "    sents_list.append(sent.text.strip())\n",
    "for sent in sents_list:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jshNrYkOzjV"
   },
   "source": [
    "## Saving sentences in a TSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7wVyd1QROzjW"
   },
   "outputs": [],
   "source": [
    "# Specify a filename for the output TSV file; 'w' below means we are opening it for writing (not reading)\n",
    "output = codecs.open('my_sentences.tsv', 'w', encoding = 'utf-8')\n",
    "\n",
    "# start a counter for sentences\n",
    "counter = 1\n",
    "\n",
    "# for every sentence\n",
    "for sent in annotations.sents:\n",
    "    # write one line containing its (arbitrarily given) identifier, type, start offset, end offset and name\n",
    "    output.write('T' + str(counter) + '\\t' + 'Sentence' + ' ' + str(sent.start_char) + ' ' + str(sent.end_char) + '\\t' + sent.text.replace('\\n', ' ') + '\\n')\n",
    "    \n",
    "    #increase the counter by 1\n",
    "    counter = counter + 1\n",
    "\n",
    "# close the file\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3TJqhmhOzjW"
   },
   "source": [
    "Check the output contents of the output file. You can do this by:\n",
    "- accessing the Jupyter dashboard again, which should now contain the output TSV file\n",
    "- clicking on the filename (which will open it directly), or\n",
    "- ticking on the box beside the filename and clicking *Download* (which will save a copy on your machine, that you can then open using a spreadsheet program later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwRjNCcAOzjW"
   },
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4dlGV6a3OzjW",
    "outputId": "638b3659-844a-4cca-e578-1b2259036583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INGREDIENTS', '\\n', 'For', 'the', 'pizza', 'base:', 'butter,', 'ghee', 'or', 'coconut', 'oil,', 'for', 'greasing;']\n",
      "['140g', 'cauliflower', '(about', '1/4', 'of', 'a', 'head', 'without', 'the', 'stalk);']\n",
      "['1', 'egg', 'white,', 'beaten;']\n",
      "['50g', ' ', 'ground', 'almonds;']\n",
      "['40g', 'buckwheat', 'flour;']\n",
      "['1/2', 'tsp', 'sea', 'salt;']\n",
      "['1/2', 'tsp', 'black', 'pepper;']\n",
      "['1/4', 'tsp', 'bicarbonate', 'of', 'soda', '\\n', 'For', 'the', 'topping:', '1', 'medium', 'mozzarella', 'ball;']\n",
      "['2', 'handfuls', 'fresh', 'tomatoes', '(a', 'mixture', 'of', 'colours', 'look', 'good);']\n",
      "['chilli', 'flakes', '(optional);']\n",
      "['handful', 'fresh', 'basil;']\n",
      "['drizzle', 'of', 'olive', 'oil,', 'to', 'serve', '\\n', 'METHOD', '\\n', '1.']\n",
      "['Preheat', 'the', 'oven', 'to', '170C/190C', 'fan/350F/Gas', '4.']\n",
      "['Line', 'a', 'baking', 'tray', 'with', 'baking', 'parchment...', 'lightly', 'grease', 'with', 'butter,', 'ghee', 'or', 'coconut', 'oil.']\n",
      "['2.']\n",
      "['Grate', 'the', 'cauliflower', 'into', 'rice-sized', 'pieces', 'using', 'a', 'hand', 'grater', 'or', 'food', 'processor.']\n",
      "['3.']\n",
      "['Put', 'all', 'the', 'pizza', 'base', 'ingredients', 'in', 'a', 'bowl...', 'mix', 'well', 'with', 'a', 'spoon,', 'or', 'add', 'to', 'the', 'food', 'processor', 'and', 'blend,', 'to', 'form', 'a', 'sticky', 'dough.']\n",
      "['4.']\n",
      "['Using', 'the', 'back', 'of', 'a', 'spoon,', 'spread', 'the', 'dough', 'out', 'onto', 'the', 'greased', 'parchment', 'on', 'the', 'tray,', 'shaping', 'it', 'into', 'a', 'circle', '30cm/12in', 'wide.']\n",
      "['5.']\n",
      "['Bake', 'in', 'the', 'oven', 'for', 'about', 'five', 'minutes;']\n",
      "['flip', 'it', 'over', 'and', 'return', 'to', 'the', 'oven', 'for', 'another', 'two', 'minutes.']\n",
      "['6.']\n",
      "['Pull', 'the', 'mozzarella', 'ball', 'apart', 'into', 'small', 'pieces', 'and', 'arrange', 'over', 'the', 'base', 'of', 'the', 'pizza.']\n",
      "['7.']\n",
      "['Slice', 'the', 'tomatoes', 'into', '1cm', 'thick', 'slices', 'and', 'arrange', 'over', 'the', 'pizza;']\n",
      "['add', 'the', 'chilli', 'flakes,', 'if', 'desired.']\n",
      "['8.']\n",
      "['Bake', 'the', 'pizza', 'for', '10-15', 'minutes', 'more', 'until', 'the', 'mozzarella', 'is', 'melted', 'and', 'bubbling.']\n",
      "['9.']\n",
      "['Sprinkle', 'the', 'basil', 'over', 'the', 'top', 'of', 'the', 'pizza', 'before', 'serving', 'and', 'drizzle', 'with', 'olive', 'oil.']\n",
      "['10.']\n",
      "['Enjoy!', 'Bon', 'appetit!']\n"
     ]
    }
   ],
   "source": [
    "# Create a tokeniser with just the default vocabulary\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# For every sentence resulting from sentence segmentation above:\n",
    "for sentence in sents_list:\n",
    "    # Apply tokenisation\n",
    "    annotations = tokenizer(sentence)\n",
    "    token_list = []\n",
    "    for token in annotations:\n",
    "        token_list.append(token.text)\n",
    "    print(token_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jurpPZyQOzjX"
   },
   "source": [
    "### Activity 2a: Are you satisfied with the tokens obtained above? Have you found any tokens which actually contain two words?\n",
    "\n",
    "**Write down your answers here:**\n",
    "<br> appetit! spoon, good);\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Activity 2b: Run the code below (a bit similar to the one above but using a tokeniser that uses punctuation rules). Observe how the results differ from that of the first tokeniser.\n",
    "\n",
    "**Write down any observations here:**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "S8Z5erJhOzjX",
    "outputId": "cd78ba44-8461-4423-cbe1-c7c8ec14120b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INGREDIENTS', '\\n', 'For', 'the', 'pizza', 'base', ':', 'butter', ',', 'ghee', 'or', 'coconut', 'oil', ',', 'for', 'greasing', ';']\n",
      "['140', 'g', 'cauliflower', '(', 'about', '1/4', 'of', 'a', 'head', 'without', 'the', 'stalk', ')', ';']\n",
      "['1', 'egg', 'white', ',', 'beaten', ';']\n",
      "['50', 'g', ' ', 'ground', 'almonds', ';']\n",
      "['40', 'g', 'buckwheat', 'flour', ';']\n",
      "['1/2', 'tsp', 'sea', 'salt', ';']\n",
      "['1/2', 'tsp', 'black', 'pepper', ';']\n",
      "['1/4', 'tsp', 'bicarbonate', 'of', 'soda', '\\n', 'For', 'the', 'topping', ':', '1', 'medium', 'mozzarella', 'ball', ';']\n",
      "['2', 'handfuls', 'fresh', 'tomatoes', '(', 'a', 'mixture', 'of', 'colours', 'look', 'good', ')', ';']\n",
      "['chilli', 'flakes', '(', 'optional', ')', ';']\n",
      "['handful', 'fresh', 'basil', ';']\n",
      "['drizzle', 'of', 'olive', 'oil', ',', 'to', 'serve', '\\n', 'METHOD', '\\n', '1', '.']\n",
      "['Preheat', 'the', 'oven', 'to', '170C/190C', 'fan/350F', '/', 'Gas', '4', '.']\n",
      "['Line', 'a', 'baking', 'tray', 'with', 'baking', 'parchment', '...', 'lightly', 'grease', 'with', 'butter', ',', 'ghee', 'or', 'coconut', 'oil', '.']\n",
      "['2', '.']\n",
      "['Grate', 'the', 'cauliflower', 'into', 'rice', '-', 'sized', 'pieces', 'using', 'a', 'hand', 'grater', 'or', 'food', 'processor', '.']\n",
      "['3', '.']\n",
      "['Put', 'all', 'the', 'pizza', 'base', 'ingredients', 'in', 'a', 'bowl', '...', 'mix', 'well', 'with', 'a', 'spoon', ',', 'or', 'add', 'to', 'the', 'food', 'processor', 'and', 'blend', ',', 'to', 'form', 'a', 'sticky', 'dough', '.']\n",
      "['4', '.']\n",
      "['Using', 'the', 'back', 'of', 'a', 'spoon', ',', 'spread', 'the', 'dough', 'out', 'onto', 'the', 'greased', 'parchment', 'on', 'the', 'tray', ',', 'shaping', 'it', 'into', 'a', 'circle', '30cm/12', 'in', 'wide', '.']\n",
      "['5', '.']\n",
      "['Bake', 'in', 'the', 'oven', 'for', 'about', 'five', 'minutes', ';']\n",
      "['flip', 'it', 'over', 'and', 'return', 'to', 'the', 'oven', 'for', 'another', 'two', 'minutes', '.']\n",
      "['6', '.']\n",
      "['Pull', 'the', 'mozzarella', 'ball', 'apart', 'into', 'small', 'pieces', 'and', 'arrange', 'over', 'the', 'base', 'of', 'the', 'pizza', '.']\n",
      "['7', '.']\n",
      "['Slice', 'the', 'tomatoes', 'into', '1', 'cm', 'thick', 'slices', 'and', 'arrange', 'over', 'the', 'pizza', ';']\n",
      "['add', 'the', 'chilli', 'flakes', ',', 'if', 'desired', '.']\n",
      "['8', '.']\n",
      "['Bake', 'the', 'pizza', 'for', '10', '-', '15', 'minutes', 'more', 'until', 'the', 'mozzarella', 'is', 'melted', 'and', 'bubbling', '.']\n",
      "['9', '.']\n",
      "['Sprinkle', 'the', 'basil', 'over', 'the', 'top', 'of', 'the', 'pizza', 'before', 'serving', 'and', 'drizzle', 'with', 'olive', 'oil', '.']\n",
      "['10', '.']\n",
      "['Enjoy', '!', 'Bon', 'appetit', '!']\n"
     ]
    }
   ],
   "source": [
    "# Create another tokeniser, this time with settings that include punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "# For every sentence resulting from sentence segmentation above:\n",
    "for sentence in sents_list:\n",
    "    # Apply tokenisation\n",
    "    annotations = tokenizer(sentence)\n",
    "    token_list = []\n",
    "    for token in annotations:\n",
    "        token_list.append(token.text)\n",
    "    print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFGUpP8uOzjX"
   },
   "source": [
    "### If you wish to try making your own tokeniser with custom rules and exceptions, you can check the **spaCy** documentation: https://spacy.io/usage/linguistic-features#native-tokenizers"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Week1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
