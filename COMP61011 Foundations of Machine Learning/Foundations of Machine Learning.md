### 1.Terminology and Rules

![Screenshot 2020-10-27 at 23.48.28](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-10-27 at 23.48.28.png)



### 2.Bayes

![Screenshot 2020-11-01 at 00.47.45](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-01 at 00.47.45.png)

### 3.Bayesian Networks

![Screenshot 2020-11-01 at 01.06.08](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-01 at 01.06.08.png)



![Screenshot 2020-11-04 at 11.34.50](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-04 at 11.34.50.png)

### 4.Learning classification model

![Screenshot 2020-11-04 at 10.49.19](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-04 at 10.49.19.png)

#### Perception

![Screenshot 2020-11-04 at 10.52.57](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-04 at 10.52.57.png)

![Screenshot 2020-11-04 at 11.21.36](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-04 at 11.21.36.png)

![Screenshot 2020-11-04 at 11.22.48](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-04 at 11.22.48.png)

![Screenshot 2020-11-05 at 10.11.36](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-05 at 10.11.36.png)

### 5. Support Vector Machines

#### p1.Introduction to Support Vector Machines

can solve non-linearly seperable problems.

Invented by Vladmir Vapnik

![Screenshot 2020-11-09 at 22.47.39](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.47.39.png)

distance to decision boundary is the best choice to select classifier.

the point to the line is margin

![Screenshot 2020-11-09 at 22.49.53](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.49.53.png)

![Screenshot 2020-11-09 at 22.50.33](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.50.33.png)

![Screenshot 2020-11-09 at 22.52.11](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.52.11.png)

![Screenshot 2020-11-09 at 22.58.41](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.58.41.png)



**Summary**

![Screenshot 2020-11-09 at 22.59.52](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 22.59.52.png)



#### p2.Liner Support Vector Machines

![Screenshot 2020-11-09 at 23.02.21](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.02.21.png)

![Screenshot 2020-11-09 at 23.06.36](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.06.36.png)

![Screenshot 2020-11-09 at 23.07.16](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.07.16.png)

![Screenshot 2020-11-09 at 23.07.32](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.07.32.png)

![Screenshot 2020-11-09 at 23.08.32](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.08.32.png)

![Screenshot 2020-11-09 at 23.10.58](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.10.58.png)



when is too big

![Screenshot 2020-11-09 at 23.11.31](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.11.31.png)



**you need a penalty function to limit the margin and find the best one !**

![Screenshot 2020-11-09 at 23.14.46](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.14.46.png)

![Screenshot 2020-11-09 at 23.17.38](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.17.38.png)

![Screenshot 2020-11-09 at 23.18.58](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.18.58.png)

![Screenshot 2020-11-09 at 23.20.59](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-09 at 23.20.59.png)



#### p3.Robust SVMs using slack variables

![Screenshot 2020-11-10 at 18.49.16](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 18.49.16.png)



allow some slack.

![Screenshot 2020-11-10 at 18.50.41](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 18.50.41.png)

![Screenshot 2020-11-10 at 18.52.15](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 18.52.15.png)

![Screenshot 2020-11-10 at 18.54.52](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 18.54.52.png)



#### p4.nonlinear SVMs

![Screenshot 2020-11-10 at 19.06.48](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.06.48.png)

![Screenshot 2020-11-10 at 19.08.29](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.08.29.png)



#### P5.non-linear SVMs in detail: the kernel trick

![Screenshot 2020-11-10 at 19.13.31](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.13.31.png)

![Screenshot 2020-11-10 at 19.14.50](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.14.50.png)

![Screenshot 2020-11-10 at 19.17.07](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.17.07.png)

![Screenshot 2020-11-10 at 19.27.00](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.27.00.png)

![Screenshot 2020-11-10 at 19.28.48](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 19.28.48.png)

### 5.SVM YouTube

![Screenshot 2020-11-10 at 23.43.20](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 23.43.20.png)

![Screenshot 2020-11-10 at 23.56.16](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 23.56.16.png)

![Screenshot 2020-11-10 at 23.57.15](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-10 at 23.57.15.png)



SVM - 

before  1-Dimension

![Screenshot 2020-11-11 at 00.04.38](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.04.38.png)

after 2-Dimensions

![Screenshot 2020-11-11 at 00.05.32](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.05.32.png)

we can use this model to classify new observation

![Screenshot 2020-11-11 at 00.06.51](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.06.51.png)



**The idea behind SVM**

1. Start with data in a relatively low dimension
2. move the data into a higher dimension
3. Find a Support Vector Classifier that separates the higher dimensional data into two groups.

Why we use x^2? how to find a higher dimension?

**use Polynomial Kernel**

![Screenshot 2020-11-11 at 00.21.54](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.21.54.png)

![Screenshot 2020-11-11 at 00.22.31](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.22.31.png)

the **Polynomial Kernel increses dimensions by setting d** to find a **SV classifier**, we can find a good value for d with **Cross Validation**

Another commonly used Kernel is **Radial Kernel**, also known as the **Radial Basis Function(RBF) Kernel**

![Screenshot 2020-11-11 at 00.29.18](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.29.18.png)

**The Kernel Trick**

![Screenshot 2020-11-11 at 00.29.56](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.29.56.png)

Why Kernel?

![Screenshot 2020-11-11 at 00.31.23](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-11 at 00.31.23.png)

### 6.Decision Tree

#### 1.Introduction

![Screenshot 2020-11-18 at 14.01.05](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 14.01.05.png)

Q. where is a good threshold?

entropy!

Q. How many layers a tree should have?

Overfitiing = fine tuning

![Screenshot 2020-11-18 at 14.10.11](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 14.10.11.png)



#### 2.Creating 'best' decisions via entropy

try splitting the data(build a decision stump)

calculate the cost for this stump

pick feature with maximum information gain

![Screenshot 2020-11-18 at 14.46.32](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 14.46.32.png)

when Pr(X=1) = 0.5, it means information is useless, so the entropy H(X) = maximum.

**So the less the entropy is, the more information you get**

you should test each feature and each threshold then select the one which makes entropy the less.

#### 3.Decision tree pruning

Q. What is Pruning?

Modifying a decision tree which has **poorer** performance on a training data set so that it has **better** performance on a validation/ test set.

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 15.11.25.png" alt="Screenshot 2020-11-18 at 15.11.25" style="zoom: 33%;" />

##### **1.Pre-pruning**

 (Early Stopping) - as we train the decision tree, decide whether it is 'worth' adding in an extra decision. (Alternative: stop splitting when information gain is below a given threshold)

**Pro**

- Does **not require validation data**
- Fast to train- no unnecessary branches are created

**Con**

- No gurantee that we get the best answer(Possible for one split to have low information gain, but subsequent splits to have high information gain)

##### **2.Post-pruning** 

post-pruning via cross validation

- Allow the decision tree to overfit to the training data (perfect classifier)
- Prune back the tree so that it still performs well on validation data



**Process**

**1.Training**

1. Build treee using training data
2. keep adding decisions until all tree is as accurate as possible

Result: complex tree, with **high accuracy** on the training set, but **low accuracy** on a validation set

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 16.14.44.png" alt="Screenshot 2020-11-18 at 16.14.44" style="zoom:33%;" />

**2.Validate**

1. Remove a branch, starting at the bottom of the tree
2. Calculate validation set error
3. If validation set error has decreased, goto step 1
4. Otherwise, 

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 16.17.54.png" alt="Screenshot 2020-11-18 at 16.17.54" style="zoom:33%;" />

**3.Test**

**Pros**

- Possible to guarantee the 'best' answer(but still very difficult for large tree)

**Cons**

- Two-stage process, so slower than pre-pruning
- Typically **requires validation data**( which may mean that it is not possible for smaller datasets)

##### 3.Summary

- Pruning- method of editing a decision tree so that it gives better generalisability
- Two main types of methods(Pre and Post)

**Pre-pruning**

- Fast
- Works well in most cases (but not always)

**Post-pruning**

- Analogous(类似) to hyperparameter optimisation for other ML classifiers
- Requires computation of 'unnecessary' branches

### 7.Practical Consideration in ML

#### 1.Data Normalization

We need to make sure each feature has a similar amount of influence on the model .

------

**Q:When does it matter**

```
Any classification method that uses distances (nearest neighbour)

SVMs (affects robustness for linear SVMs, and affects the decision boundary for non-linear SCMs)

Caveat: might not scale, if the relative size of features is important. (sometimes you cannot normalize one data because it is relative to other data)
```

------

**Q: How?**

**x.norm = (x - x.average)/sigma**         calculate sigma by standard deviation

**x.norm = (x-x.min)/(x.max-x.min)**

tips: calculate mean and variance of training data, **NOT** the whole set

![Screenshot 2020-11-18 at 16.51.40](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 16.51.40.png)



------

#### 2.Algorithm Convergence

For most ML algorithms, we perform an iterative process to train a model (exception: K-NN)

------

Q.When should we stop training

- minimal decrease in error/loss
- increase in erros/loss

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 16.57.37.png" alt="Screenshot 2020-11-18 at 16.57.37" style="zoom:33%;" />

------

#### 3.Data Leakage

Data leakage occurs when we  accidentally use information from the test set when we are training a machine model. It leads to artificially better performance in the test data. 

variance 方差

------

**Data Leakage example(normalisation)**

![Screenshot 2020-11-18 at 17.57.25](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 17.57.25.png)

Test data is scaled according to the distribution of the training data,only. Test data is far from zero, so it will be classified as **abnormal**.

![Screenshot 2020-11-18 at 17.59.38](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 17.59.38.png)

Test data is sclaed according to **the combined distribution of the training and test data**. Much less of the test data is far from zero, and so the classifier decides a greater proportion of meansurements are **normal**.

------

**Data Leakage in Cross Validation**

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 18.03.36.png" alt="Screenshot 2020-11-18 at 18.03.36" style="zoom: 25%;" />

#### 4. Imbalanced Learning

------



- Imbalance is common
- Extreme imbalance common for many real-life problems:
  - Credit card fraud
  - Health monitoring

- Accuracy paradox
  - Possible to get very hign accuracy with a poor classifier
  - eg. Consider data set with:
    - 1,000,000 normal credit card transactions
    - 10 fraud

------

- Undersampling
  - Randomly remove samples of the majority class
  - E.g. if 60,000 in class 1, and 10000 in class 2, then remove 50,000 from the class 1 examples at random.

- Oversampling
  - Create extra instances of the minority class
  - E.g.if 60,000 in class 1,and 10,000 in class 2, sample randomly 50,000 times from examples in class 2, with replacement, and add to the training set.
  - Data augmentation

------

**Oversampling via data augmentation**

- Synthetic(人造) Minority Over-Sampling Technique

  creates samples of the minority class that are 'similar' to the training data

- For some problems, we can safely create extra instance of the minority class.

  ![Screenshot 2020-11-18 at 18.16.02](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 18.16.02.png)

### 8.Receiver operating characteristics（ROC）

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 23.24.05.png" alt="Screenshot 2020-11-18 at 23.24.05" style="zoom:50%;" />

fp rate= FP / N



tp rate = recall(召回率) = sensitivity(真阳性率) = TP/(TP+FN)= TP/p

specificity (真阴性率)= TN/n = 1 - fp rate



precision= positive predictive value = TP/(TP+FP) = TP/Y

accuracy = (TP+TN)/(p+n)

(the larger is the better) F-measure = 2/(1/precision+1/recall)

------

**ROC graph**

![Screenshot 2020-11-18 at 23.44.50](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-18 at 23.44.50.png)

tp rate is on the Y, fp rate is on the x.

point on the y=x is like just to guess randomly. A good model should on the westnorth.



（**AUC**） the area of ROC is a way measure how good the model is .





### 9.Introduction to Ensembles

![Screenshot 2020-11-24 at 23.27.53](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.27.53.png)

- **BootStrapping** is a way of creating variation in training data
  - By randomly sampling from the original data, we aim to create a new training set that is representative of the original data
- **Bagging** is the term used when we create an ensemble model using models trained on bootstrapped data (vote decision- we want each models has less correlation with the error)
- **Random Forests** apply we bagging AND random feature selection to decision trees. (forcing each tree slight worse than optimal so that we get a wider variation)

------

**Boosting(Adaboost)**

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.42.35.png" alt="Screenshot 2020-11-24 at 23.42.35" style="zoom: 33%;" />



<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.44.47.png" alt="Screenshot 2020-11-24 at 23.44.47" style="zoom:33%;" />

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.50.54.png" alt="Screenshot 2020-11-24 at 23.50.54" style="zoom: 25%;" />

Increase the number of misclassified samples so that the training data is **biased** towards them.

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.52.19.png" alt="Screenshot 2020-11-24 at 23.52.19" style="zoom: 25%;" />

Can your classifier naturally take advantage of a weighted distribution?

- yes - Boosting by "**Reweighting**" (e.g. using Naive Bayes)
- no - Boosting by "**Resampling**" (e.g. using linear discriminants)

------

**Re-weight**

- Downweight if prediction matches label
- Upweight prediction does not match label
- Amount of weight adjustment depends on prediction uncertainty.

![Screenshot 2020-11-24 at 23.59.56](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-24 at 23.59.56.png)

------

**Summary**

- Sequential ensenmble models include Adaboost, Xgboost, gradient boosted tree( using it when it is easy to overfit)
- Key idea: bias the training data set iteratively, so that it lends greater weight to samples that were previously misclassified.
- The result is **a set of weak classifiers**, that produce a good result when used as an ensemble
- Tend to work extremely well "off-the-shelf" - no parameter tuning.

------



### 10.Feature Selection

#### 1. Wrapper methods

**Overfitting**

Somtimes we have too little data( features >> examples)

------

**Exhaustive search**

2^M possible sets!



**Forward Feature Selection**

Simplest strategy : **greedy** search



Greedy forward search evaluates M(M+1)/2 Sets.

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 14.26.20.png" alt="Screenshot 2020-11-27 at 14.26.20" style="zoom:50%;" />



**Backward feature elimination**

- Strat with a classifier that uses **all** features.
- Try removing one feature from the model, in turn
- Keep the model that has least impact on "accuracy".



------

**Pros and Cons of Forward and Backward Selection**

- No guarantee of best solution
- Need to train and evalueate lots of models



- Impact of a feature on the ML classifier is explicit
- Better than an exhaustive search

------





#### 2.Filter Methods

------

**Correlation Coefficient**

<img src="/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 16.36.14.png" alt="Screenshot 2020-11-27 at 16.36.14" style="zoom:67%;" />



you calculate the correlation of each feature and select the one with **lowest validation error**



**Limitations of coorelation**

![Screenshot 2020-11-27 at 16.41.56](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 16.41.56.png)

------

**Fisher Score**

![Screenshot 2020-11-27 at 16.48.22](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 16.48.22.png)

We want a larger F!



Two irrelevant features may be relevant together

![Screenshot 2020-11-27 at 16.51.44](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 16.51.44.png)



![Screenshot 2020-11-27 at 16.57.37](/Users/guohuanjie/Documents/UoM Learning/COMP61011 Foundations of Machine Learning/image/Screenshot 2020-11-27 at 16.57.37.png)





------

**Filter vs wrapper**

- Filter methods quantify the relevance of features using statistics about the data
- Much faster(+ scales better to larger data sets)
- No guarantee that redundant features won't be selected



**Filter**

- pro: fast and easy
- con: hard to find the combinated effects of the features

**Wrapper**

- pro: optimize the result through ml algorithm
- con: large searching zoom, you need to design a seaching strategy to make it faster.

------

